# Logstash Configuration for Enterprise Monitoring
input {
  # File input for application logs
  file {
    path => "/var/log/gongbu/*.log"
    type => "application"
    codec => "json"
    start_position => "beginning"
    sincedb_path => "/var/lib/logstash/sincedb"
  }
  
  # Beats input for system metrics
  beats {
    port => 5044
    type => "beats"
  }
  
  # TCP input for structured logs
  tcp {
    port => 5000
    codec => "json_lines"
    type => "tcp"
  }
  
  # UDP input for syslog
  udp {
    port => 514
    type => "syslog"
  }
  
  # HTTP input for webhook logs
  http {
    port => 8080
    type => "http"
    codec => "json"
  }
}

filter {
  # Parse application logs
  if [type] == "application" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{DATA:service}: %{GREEDYDATA:log_message}" }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "environment" => "production" }
      add_field => { "platform" => "gongbu" }
    }
  }
  
  # Parse system metrics
  if [type] == "beats" {
    if [fields][service] {
      mutate {
        add_field => { "service_name" => "%{[fields][service]}" }
      }
    }
  }
  
  # Parse security logs
  if [type] == "security" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:severity}\] %{DATA:event_type}: %{GREEDYDATA:details}" }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "log_type" => "security" }
    }
  }
  
  # Parse database logs
  if [type] == "database" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{DATA:database}: %{GREEDYDATA:query}" }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "log_type" => "database" }
    }
  }
  
  # Parse nginx logs
  if [type] == "nginx" {
    grok {
      match => { "message" => "%{NGINXACCESS}" }
    }
    
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    mutate {
      add_field => { "log_type" => "nginx" }
    }
  }
  
  # Parse redis logs
  if [type] == "redis" {
    grok {
      match => { "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{DATA:level}\] %{GREEDYDATA:redis_message}" }
    }
    
    date {
      match => [ "timestamp", "ISO8601" ]
    }
    
    mutate {
      add_field => { "log_type" => "redis" }
    }
  }
  
  # Common filters for all logs
  mutate {
    remove_field => [ "host", "agent", "ecs", "input", "log" ]
  }
  
  # Add geoip for IP addresses
  if [client_ip] {
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # Add user agent parsing
  if [user_agent] {
    useragent {
      source => "user_agent"
      target => "user_agent_parsed"
    }
  }
  
  # Add timestamp
  mutate {
    add_field => { "processed_at" => "%{@timestamp}" }
  }
}

output {
  # Output to Elasticsearch
  elasticsearch {
    hosts => ["elasticsearch:9200"]
    index => "gongbu-logs-%{+YYYY.MM.dd}"
    template_name => "gongbu-logs"
    template => "/usr/share/logstash/templates/gongbu-logs.json"
    template_overwrite => true
    user => "elastic"
    password => "enterprise_elasticsearch_password"
    ssl => true
    ssl_certificate_verification => false
  }
  
  # Output to file for debugging
  file {
    path => "/var/log/logstash/processed-logs.log"
    codec => "json_lines"
  }
  
  # Output to stdout for debugging
  stdout {
    codec => "rubydebug"
  }
}
